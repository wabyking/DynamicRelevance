<set>
<record>
		<id>6</id>
		<query>NoSQL Database</query>
		<desicription>
			A NoSQL (originally referring to "non SQL" or "non relational" ) database provides a mechanism for storage and retrieval of data that is modeled in means other than the tabular relations used in relational databases. Such databases have existed since the late 1960s, but did not obtain the "NoSQL" moniker until a surge of popularity in the early twenty-first century, triggered by the needs of Web 2.0 companies such as Facebook, Google and Amazon.com.
			Motivations for this approach include: simplicity of design, simpler "horizontal" scaling to clusters of machines, which is a problem for relational databases,[2] and finer control over availability. The data structures used by NoSQL databases (e.g. key-value, wide column, graph, or document) differ slightly from those used by default in relational databases, making some operations faster in NoSQL and others faster in relational databases. The particular suitability of a given NoSQL database depends on the problem it must solve. Sometimes the data structures used by NoSQL databases are also viewed as "more flexible" than relational database tables.
		</desicription>
		<d1    title="database and DBMS">
			A database is an organized collection of data.[1] It is the collection of schemas, tables, queries, reports, views and other objects. The data is typically organized to model aspects of reality in a way that supports processes requiring information, such as modelling the availability of rooms in hotels in a way that supports finding a hotel with vacancies.
			A database management system (DBMS) is a computer software application that interacts with the user, other applications, and the database itself to capture and analyze data. A general-purpose DBMS is designed to allow the definition, creation, querying, update, and administration of databases. Well-known DBMSs include MySQL, PostgreSQL, Microsoft SQL Server, Oracle, Sybase and IBM DB2. A database is not generally portable across different DBMSs, but different DBMS can interoperate by using standards such as SQL and ODBC or JDBC to allow a single application to work with more than one DBMS. Database management systems are often classified according to the database model that they support; the most popular database systems since the 1980s have all supported the relational model as represented by the SQL language.[disputed – discuss] Sometimes a DBMS is loosely referred to as a 'database'.

		</d1>
		<d2  title="mangodb">
			MongoDB (from humongous) is a cross-platform document-oriented database. Classified as a NoSQL database, MongoDB eschews the traditional table-based relational database structure in favor of JSON-like documents with dynamic schemas (MongoDB calls the format BSON), making the integration of data in certain types of applications easier and faster. Released under a combination of the GNU Affero General Public License and the Apache License, MongoDB is free and open-source software.
			First developed by the software company MongoDB Inc. in October 2007 as a component of a planned platform as a service product, the company shifted to an open source development model in 2009, with MongoDB offering commercial support and other services.[2] Since then, MongoDB has been adopted as backend software by a number of major websites and services, including Craigslist, eBay, and Foursquare among others. As of July 2015, MongoDB is the fourth most popular type of database management system, and the most popular for document stores.
			
		</d2>
</record>

<record>
		<id>7</id>
		<query>Machine learning</query>
		<desicription>
			Machine learning is a subfield of computer science[1] that evolved from the study of pattern recognition and computational learning theory in artificial intelligence.[1] Machine learning explores the study and construction of algorithms that can learn from and make predictions on data.[2] Such algorithms operate by building a model from example inputs in order to make data-driven predictions or decisions,[3]:2 rather than following strictly static program instructions.

			Machine learning is closely related to computational statistics; a discipline that aims at the design of algorithm for implementing statistical methods on computers. It has strong ties to mathematical optimization, which delivers methods, theory and application domains to the field. Machine learning is employed in a range of computing tasks where designing and programming explicit algorithms is infeasible. Example applications include spam filtering, optical character recognition (OCR),[4] search engines and computer vision. Machine learning is sometimes conflated with data mining,[5] although that focuses more on exploratory data analysis.[6] Machine learning and pattern recognition "can be viewed as two facets of the same field."[3]:vii
		</desicription>
		<d1 title="Artificial intelligence">
			Artificial intelligence (AI) is the intelligence exhibited by machines or software. It is also the name of the academic field of study which studies how to create computers and computer software that are capable of intelligent behavior. Major AI researchers and textbooks define this field as "the study and design of intelligent agents",[1] in which an intelligent agent is a system that perceives its environment and takes actions that maximize its chances of success.[2] John McCarthy, who coined the term in 1955,[3] defines it as "the science and engineering of making intelligent machines".[4]
			The central problems (or goals) of AI research include reasoning, knowledge, planning, learning, natural language processing (communication), perception and the ability to move and manipulate objects.[6] General intelligence is still among the field's long-term goals.[7] Currently popular approaches include statistical methods, computational intelligence and traditional symbolic AI. There are a large number of tools used in AI, including versions of search and mathematical optimization, logic, methods based on probability and economics, and many others. The AI field is interdisciplinary, in which a number of sciences and professions converge, including computer science, mathematics, psychology, linguistics, philosophy and neuroscience, as well as other specialized fields such as artificial psychology.
			
			
		</d1>
		<d2  title="Pattern recognition">
			Pattern recognition is a branch of machine learning that focuses on the recognition of patterns and regularities in data, although it is in some cases considered to be nearly synonymous with machine learning.[1] Pattern recognition systems are in many cases trained from labeled "training" data (supervised learning), but when no labeled data are available other algorithms can be used to discover previously unknown patterns (unsupervised learning).

			The terms pattern recognition, machine learning, data mining and knowledge discovery in databases (KDD) are hard to separate, as they largely overlap in their scope. Machine learning is the common term for supervised learning methods[dubious – discuss] and originates from artificial intelligence, whereas KDD and data mining have a larger focus on unsupervised methods and stronger connection to business use. Pattern recognition has its origins in engineering, and the term is popular in the context of computer vision: a leading computer vision conference is named Conference on Computer Vision and Pattern Recognition. In pattern recognition, there may be a higher interest to formalize, explain and visualize the pattern, while machine learning traditionally focuses on maximizing the recognition rates. Yet, all of these domains have evolved substantially from their roots in artificial intelligence, engineering and statistics, and they've become increasingly similar by integrating developments and ideas from each other.

			In machine learning, pattern recognition is the assignment of a label to a given input value. In statistics, discriminant analysis was introduced for this same purpose in 1936. An example of pattern recognition is classification, which attempts to assign each input value to one of a given set of classes (for example, determine whether a given email is "spam" or "non-spam"). However, pattern recognition is a more general problem that encompasses other types of output as well. Other examples are regression, which assigns a real-valued output to each input; sequence labeling, which assigns a class to each member of a sequence of values (for example, part of speech tagging, which assigns a part of speech to each word in an input sentence); and parsing, which assigns a parse tree to an input sentence, describing the syntactic structure of the sentence.[citation needed]
		</d2>
</record>

<record>
		<id>8</id>
		<query>American president</query>
		<desicription>
			The President of the United States of America (POTUS)[7] is the elected head of state and head of government of the United States. The president leads the executive branch of the federal government and is the commander-in-chief of the United States Armed Forces.

			The President of the United States is considered one of the world's most powerful people.[8][9][10][11] The role includes being the commander-in-chief of the world's most expensive military with the largest nuclear arsenal and leading the nation having the largest economy by real and nominal GDP, described as the world's only contemporary superpower. The office of the president holds significant hard and soft power both in the United States and abroad.
		</desicription>
		<d1 title="Barack Obama">
			Barack Hussein Obama II (US Listeni/bəˈrɑːk huːˈseɪn ɵˈbɑːmə/; born August 4, 1961) is the 44th and current President of the United States, as well as the first African American to hold the office. Born in Honolulu, Hawaii, Obama is a graduate of Columbia University and Harvard Law School, where he served as president of the Harvard Law Review. He was a community organizer in Chicago before earning his law degree. He worked as a civil rights attorney and taught constitutional law at University of Chicago Law School between 1992 and 2004. He served three terms representing the 13th District in the Illinois Senate from 1997 to 2004, running unsuccessfully for the United States House of Representatives in 2000 against Bobby Rush.

			In 2004, Obama received national attention during his campaign to represent Illinois in the United States Senate with his victory in the March Democratic Party primary, his keynote address at the Democratic National Convention in July, and his election to the Senate in November. He began his presidential campaign in 2007 and, after a close primary campaign against Hillary Rodham Clinton in 2008, he won sufficient delegates in the Democratic Party primaries to receive the presidential nomination. He then defeated Republican nominee John McCain in the general election, and was inaugurated as president on January 20, 2009. Nine months after his inauguration, Obama was named the 2009 Nobel Peace Prize laureate.
			
		</d1>
		<d2 title="The separation of powers">
			The separation of powers, often imprecisely used interchangeably with the trias politica principle,[1] is a model for the governance of a state (or who controls the state). The model was first developed in Congress. Under this model, the state is divided into branches, each with separate and independent powers and areas of responsibility so that the powers of one branch are not in conflict with the powers associated with the other branches. The typical division of branches is into a legislature, an executive, and a judiciary. It can be contrasted with the fusion of powers in a parliamentary system where the executive and legislature (and sometimes parts of the judiciary) are unified.
			To prevent one branch from becoming supreme, protect the "opulent minority" from the majority, and to induce the branches to cooperate, government systems that employ a separation of powers need a way to balance each of the branches. Typically this was accomplished through a system of "checks and balances", the origin of which, like separation of powers itself, is specifically credited to Montesquieu. Checks and balances allow for a system-based regulation that allows one branch to limit another, such as the power of the United States Congress to alter the composition and jurisdiction of the federal courts. Both bipartite and tripartite governmental systems apply the principles of the separation of powers to allow for the branches represented by the separate powers to hold each other reciprocally responsible to the assertion of powers as apportioned by law. The following example of the separation of powers and their mutual checks and balances for the experience of the United States Constitution is presented as illustrative of the general principles applied in similar forms of government as well.
		</d2>
</record>


<record>
		<id>9</id>
		<query>Turing Award</query>
		<desicription>
			The ACM A.M. Turing Award is an annual prize given by the Association for Computing Machinery (ACM) to "an individual selected for contributions of a technical nature made to the computing community". It is stipulated that "The contributions should be of lasting and major technical importance to the computer field".[2] The Turing Award is generally recognized as the highest distinction in computer science[3][4] and the "Nobel Prize of computing".
		</desicription>
		<d1 title="Andrew Chi-Chih Yao">			
			Andrew Chi-Chih Yao (Chinese: 姚期智; pinyin: Yáo Qīzhì) is a Chinese American computer scientist and computational theorist. Yao used the minimax theorem to prove what is now known as Yao's Principle.
			In 1996 he was awarded the Knuth Prize. He received the Turing Award, the most prestigious award in computer science, in 2000, "in recognition of his fundamental contributions to the theory of computation, including the complexity-based theory of pseudorandom number generation, cryptography, and communication complexity".
			From 1982 to 1986, he was a full professor at Stanford University. From 1986 to 2004, he was the William and Edna Macaleer Professor of Engineering and Applied Science at Princeton University, where he continued to work on algorithms and complexity. In 2004, he became a Professor of the Center for Advanced Study, Tsinghua University (CASTU) and the director of the Institute for Theoretical Computer Science (ITCS), Tsinghua University in Beijing. Since 2010, he has served as the dean of Institute for Interdisciplinary Information Sciences (IIIS) in Tsinghua University. He is also the Distinguished Professor-at-Large in the Chinese University of Hong Kong.
		</d1>
		<d2 title="Alan Turing">
			
			Alan Mathison Turing, OBE, FRS (/ˈtjʊərɪŋ/; 23 June 1912 – 7 June 1954) was a British pioneering computer scientist, mathematician, logician, cryptanalyst and theoretical biologist. He was highly influential in the development of computer science, providing a formalisation of the concepts of algorithm and computation with the Turing machine, which can be considered a model of a general purpose computer.[2][3][4] Turing is widely considered to be the father of theoretical computer science and artificial intelligence.[5]

			During the Second World War, Turing worked for the Government Code and Cypher School (GC&CS) at Bletchley Park, Britain's codebreaking centre. For a time he led Hut 8, the section responsible for German naval cryptanalysis. He devised a number of techniques for breaking German ciphers, including improvements to the pre-war Polish bombe method and an electromechanical machine that could find settings for the Enigma machine. Turing played a pivotal role in cracking intercepted coded messages that enabled the Allies to defeat the Nazis in many crucial engagements, including the Battle of the Atlantic; it has been estimated that this work shortened the war in Europe by as many as two to four years.[6]

			After the war, he worked at the National Physical Laboratory, where he designed the ACE, among the first designs for a stored-program computer. In 1948 Turing joined Max Newman's Computing Laboratory at the University of Manchester, where he helped develop the Manchester computers[7] and became interested in mathematical biology. He wrote a paper on the chemical basis of morphogenesis, and predicted oscillating chemical reactions such as the Belousov–Zhabotinsky reaction, first observed in the 1960s.

		</d2>
</record>


<record>
		<id>10</id>
		<query>deep learning</query>
		<desicription>
			Deep learning (deep machine learning, or deep structured learning, or hierarchical learning, or sometimes DL) is a branch of machine learning based on a set of algorithms that attempt to model high-level abstractions in data by using multiple processing layers with complex structures, or otherwise composed of multiple non-linear transformations
		</desicription>
		<d1 title="Deep belief network">
			In machine learning, a deep belief network (DBN) is a generative graphical model, or alternatively a type of deep neural network, composed of multiple layers of latent variables ("hidden units"), with connections between the layers but not between units within each layer.[1]
			Schematic overview of a deep belief net. Arrows represent directed connections in the graphical model that the net represents.
			When trained on a set of examples in an unsupervised way, a DBN can learn to probabilistically reconstruct its inputs. The layers then act as feature detectors on inputs.[1] After this learning step, a DBN can be further trained in a supervised way to perform classification.[2]
			DBNs can be viewed as a composition of simple, unsupervised networks such as restricted Boltzmann machines (RBMs)[1] or autoencoders,[3] where each sub-network's hidden layer serves as the visible layer for the next. This also leads to a fast, layer-by-layer unsupervised training procedure, where contrastive divergence is applied to each sub-network in turn, starting from the "lowest" pair of layers (the lowest visible layer being a training set).
			The observation, due to Yee-Whye Teh, Geoffrey Hinton's student,[2] that DBNs can be trained greedily, one layer at a time, has been called a breakthrough in deep learning.
			
		</d1>
		<d2 title="convolutional neural network">
			In machine learning, a convolutional neural network (CNN, or ConvNet) is a type of feed-forward artificial neural network where the individual neurons are tiled in such a way that they respond to overlapping regions in the visual field.[1] Convolutional networks were inspired by biological processes[2] and are variations of multilayer perceptrons designed to use minimal amounts of preprocessing.[3] They have wide applications in image and video recognition.
			When used for image recognition, convolutional neural networks (CNNs) consist of multiple layers of small neuron collections which look at small portions of the input image, called receptive fields. The results of these collections are then tiled so that they overlap to obtain a better representation of the original image; this is repeated for every such layer. Because of this, they are able to tolerate translation of the input image.[4] Convolutional networks may include local or global pooling layers, which combine the outputs of neuron clusters.[5][6] They also consist of various combinations of convolutional layers and fully connected layers, with pointwise nonlinearity applied at the end of or after each layer.[7] It is inspired by biological processes. To avoid the situation that there exist billions of parameters if all layers are fully connected, the idea of using a convolution operation on small regions has been introduced. One major advantage of convolutional networks is the use of shared weight in convolutional layers, which means that the same filter (weights bank) is used for each pixel in the layer; this both reduces required memory size and improves performance.[3]
		</d2>
</record>



<record>
		<id>11</id>
		<query>Semantic Web</query>
		<desicription>
			The Semantic Web is an extension of the Web through standards by the World Wide Web Consortium (W3C).[1] The standards promote common data formats and exchange protocols on the Web, most fundamentally the Resource Description Framework (RDF).
			According to the W3C, "The Semantic Web provides a common framework that allows data to be shared and reused across application, enterprise, and community boundaries".[2] The term was coined by Tim Berners-Lee for a web of data that can be processed by machines.[3] While its critics have questioned its feasibility, proponents argue that applications in industry, biology and human sciences research have already proven the validity of the original concept.
		</desicription>
		<d1 title="theology">
			Augustine of Hippo defined the Latin equivalent, theologia, as "reasoning or discussion concerning the Deity";[2] Richard Hooker defined "theology" in English as "the science of things divine".[3] The term can, however, be used for a variety of different disciplines or fields of study.[4] Theologians use various forms of analysis and argument (philosophical, ethnographic, historical, spiritual and others) to help understand, explain, test, critique, defend or promote any of myriad religious topics. Theology might be undertaken to help the theologian:
			understand more truly their own religious tradition,[5]
			understand more truly another religious tradition,[6]
			make comparisons among religious traditions,[7]
			defend or justify a religious tradition,
			facilitate reform of a particular tradition,[8]
			assist in the propagation of a religious tradition,[9] or
			draw on the resources of a tradition to address some present situation or need,[10]
			draw on the resources of a tradition to explore possible ways of interpreting the world,[11] or
			explore the nature of divinity without reference to any specific tradition.
			challenge (ex. biblical criticism) or oppose (ex. irreligion) a religious tradition or the religious world-view.

			
		</d1>
		<d2 title="ontology">
			Ontology is the philosophical study of the nature of being, becoming, existence, or reality, as well as the basic categories of being and their relations. Traditionally listed as a part of the major branch of philosophy known as metaphysics, ontology deals with questions concerning what entities exist or may be said to exist, and how such entities may be grouped, related within a hierarchy, and subdivided according to similarities and differences. Although ontology as a philosophical enterprise is highly theoretical, it also has practical application in information science and technology, such as ontology engineering.
			In analytic philosophy, ontology deals with the determination whether categories of being are fundamental and discusses in what sense the items in those categories may be said to "be". It is the inquiry into being in so much as it is being ("being qua being"), or into beings insofar as they exist—and not insofar as (for instance) particular facts may be obtained about them or particular properties belong to them.
			Some philosophers, notably of the Platonic school, contend that all nouns (including abstract nouns) refer to existent entities. Other philosophers contend that nouns do not always name entities, but that some provide a kind of shorthand for reference to a collection of either objects or events. In this latter view, mind, instead of referring to an entity, refers to a collection of mental events experienced by a person; society refers to a collection of persons with some shared characteristics, and geometry refers to a collection of a specific kind of intellectual activity.[1] Between these poles of realism and nominalism, stand a variety of other positions; but any ontology must give an account of which words refer to entities, which do not, why, and what categories result.
		</d2>
</record>



<!-- <record>
		<id></id>
		<query></query>
		<desicription>
			
		</desicription>
		<d1>
			
			
		</d1>
		<d2>
			
		</d2>
</record> -->
</set>